{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "147dd898",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7e57169e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5e7e77d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y1_train = train_data['label'].values\n",
    "X1_train = train_data.drop(columns=['label']).values/255\n",
    "X_test = np.array(X1_train[3600:4200])\n",
    "y_test = np.array(y1_train[3600:4200])\n",
    "X_train = np.array(X1_train[0:3600])\n",
    "y_train = np.array(y1_train[0:3600])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a7e26fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):  #defined relu function\n",
    "    x[x<0]=0\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1df31859",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(X_in,weights):  # defined softmax function\n",
    "    s = np.exp(np.matmul(X_in,weights))\n",
    "    total = np.sum(s, axis=1).reshape(-1,1)\n",
    "    return s / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0416d3be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(y_pred,y_true):    # defined loss function\n",
    "    global K \n",
    "    K = 10\n",
    "    N = len(y_true)\n",
    "    y_true_one_hot_vec = (y_true[:,np.newaxis] == np.arange(K))\n",
    "    loss_sample = (np.log(y_pred) * y_true_one_hot_vec).sum(axis=1)\n",
    "    return -np.mean(loss_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b2799772",
   "metadata": {},
   "outputs": [],
   "source": [
    "def h(X,W,b):      #forward propogation\n",
    "    a1 = X\n",
    "    z1 = np.matmul(X, W[0]) + b[0]\n",
    "    a2 = relu(z1)\n",
    "    z2 = np.matmul(a2, W[1]) + b[1]\n",
    "    s = np.exp(z2)\n",
    "    total = np.sum(s, axis=1).reshape(-1,1)\n",
    "    sigma = s/total\n",
    "    return sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4266b8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop(W,b,X,y,alpha=1e-4):    #backward propogation\n",
    "    a1 = X\n",
    "    z1 = np.matmul(X, W[0]) + b[0]\n",
    "    a2 = relu(z1)\n",
    "    z2 = np.matmul(a2, W[1]) + b[1]\n",
    "    s = np.exp(z2)\n",
    "    total = np.sum(s, axis=1).reshape(-1,1)\n",
    "    sigma = s/total\n",
    "    \n",
    "    K = 10\n",
    "    N = X.shape[0]\n",
    "    y_one_hot_vec = (y[:,np.newaxis] == np.arange(K))\n",
    "    delta2 = (sigma - y_one_hot_vec)\n",
    "    grad_W1 = np.matmul(a2.T, delta2)\n",
    "    \n",
    "    delta1 = np.matmul(delta2, W[1].T)*(z1>0)\n",
    "    grad_W0 = np.matmul(X.T, delta1)\n",
    "    \n",
    "    dW = [grad_W0/N + alpha*W[0], grad_W1/N + alpha*W[1]]\n",
    "    db0 = np.mean(delta1, axis=0)\n",
    "    db1 = np.mean(delta2, axis=0)\n",
    "    return dW,db0,db1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1eb189fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_iter = 2200 # number of iterations of gradient descent\n",
    "alpha = 0.04\n",
    "n_H = 256 # number of neurons in the hidden layer\n",
    "n = X_train.shape[1] # number of pixels in an image\n",
    "K = 10\n",
    "alpha = 0.001\n",
    "# np.random.seed(1127)\n",
    "W = [1e-1*np.random.randn(n, n_H),1e-1*np.random.randn(n_H, K)]\n",
    "b = [np.random.randn(n_H), np.random.randn(K)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e3463c44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final cross-entropy loss is 3.5621791\n",
      "Final cross-entropy loss is 2.2910176\n",
      "Final cross-entropy loss is 2.0214507\n",
      "Final cross-entropy loss is 1.8609841\n",
      "Final cross-entropy loss is 1.7249816\n",
      "Final cross-entropy loss is 1.6072377\n",
      "Final cross-entropy loss is 1.504924\n",
      "Final cross-entropy loss is 1.4155471\n",
      "Final cross-entropy loss is 1.3370822\n",
      "Final cross-entropy loss is 1.2679283\n",
      "Final cross-entropy loss is 1.2066258\n",
      "Final cross-entropy loss is 1.1519715\n",
      "Final cross-entropy loss is 1.103031\n",
      "Final cross-entropy loss is 1.0590393\n",
      "Final cross-entropy loss is 1.0193262\n",
      "Final cross-entropy loss is 0.9832956\n",
      "Final cross-entropy loss is 0.95049423\n",
      "Final cross-entropy loss is 0.92051146\n",
      "Final cross-entropy loss is 0.89302039\n",
      "Final cross-entropy loss is 0.86775095\n",
      "Final cross-entropy loss is 0.84442347\n",
      "Final cross-entropy loss is 0.82283401\n",
      "(10,)\n",
      "Final cross-entropy loss is 0.80301253\n",
      "Final training accuracy is 80.5278%\n",
      "[[2.67140006e-03 6.84206643e-01 4.72521178e-02 ... 7.56238810e-03\n",
      "  1.20952840e-01 1.33882465e-02]\n",
      " [9.55354090e-01 2.54953076e-04 4.38396345e-03 ... 2.16496273e-03\n",
      "  9.77651169e-03 4.57186911e-04]\n",
      " [8.81761037e-03 7.86277251e-01 1.32660104e-02 ... 2.95915460e-02\n",
      "  2.43419537e-02 3.08152751e-02]\n",
      " ...\n",
      " [1.83155292e-02 5.20202780e-02 4.16216389e-01 ... 9.98678313e-03\n",
      "  1.48739233e-01 2.52454406e-02]\n",
      " [1.71321538e-01 5.43526817e-03 2.93017589e-02 ... 3.02310115e-03\n",
      "  6.84906358e-02 7.40972445e-03]\n",
      " [8.10724416e-01 6.00520617e-03 1.14703175e-02 ... 9.63462226e-03\n",
      "  2.00246656e-02 2.39570932e-02]]\n"
     ]
    }
   ],
   "source": [
    "for i in range(num_iter):\n",
    "    dW,db0,db1 = backprop(W,b,X_train,y_train,alpha)\n",
    "    W[0] = W[0] - alpha*(dW[0])\n",
    "    W[1] = W[1] - alpha*(dW[1])\n",
    "    b[0] = b[0] - alpha*(db0)\n",
    "    b[1] = b[1] - alpha*(db1)\n",
    "    y_pred_final = h(X_train,W,b)\n",
    "    if(i%100 == 0):\n",
    "        print(\"Final cross-entropy loss is {:.8}\".format(loss(y_pred_final,y_train)))\n",
    "y_pred_final = h(X_train,W,b)\n",
    "print(db1.shape)\n",
    "print(\"Final cross-entropy loss is {:.8}\".format(loss(y_pred_final,y_train)))\n",
    "print(\"Final training accuracy is {:.4%}\".format(np.mean(np.argmax(y_pred_final, axis=1)== y_train)))\n",
    "print(y_pred_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5cf7e11b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test data accuracy =  79.66666666666666\n"
     ]
    }
   ],
   "source": [
    "sigma = h(X_test,W,b)\n",
    "sigma_new = np.argmax(sigma,axis = 1)\n",
    "print(\"test data accuracy = \",np.sum((sigma_new == y_test))/sigma_new.shape[0] * 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
